<!-- TOC -->

- [1. 引言](#1-引言)
    - [1.1. 编写目的](#11-编写目的)
    - [1.2. 项目背景](#12-项目背景)
    - [1.3. 定义(专门术语和缩写词)](#13-定义专门术语和缩写词)
- [2. 任务概述](#2-任务概述)
    - [2.1. 目标](#21-目标)
    - [2.2. 运行环境](#22-运行环境)
    - [2.3. 条件限制](#23-条件限制)
- [3. 数据描述](#3-数据描述)
    - [3.1. 静态数据](#31-静态数据)
    - [3.2. 动态数据](#32-动态数据)
    - [3.3. ES索引描述](#33-es索引描述)
    - [3.4. 索引数据字典](#34-索引数据字典)
    - [3.5. 数据采集](#35-数据采集)
- [4. 功能需求](#4-功能需求)
    - [4.1. 数据爬取与预处理功能](#41-数据爬取与预处理功能)
    - [4.2. 计算 PageRank 功能](#42-计算-pagerank-功能)
    - [4.3. AI 文本分类并提交到 ES 功能](#43-ai-文本分类并提交到-es-功能)
    - [4.4. 用户交互功能](#44-用户交互功能)
- [5. 性能需求](#5-性能需求)
    - [5.1. 数据精确度](#51-数据精确度)
    - [5.2. 时间特性](#52-时间特性)
    - [5.3. 适应性](#53-适应性)
- [6. 运行需求](#6-运行需求)
    - [6.1. 用户界面](#61-用户界面)
        - [6.1.1. 主页](#611-主页)
        - [6.1.2. 搜索结果界面](#612-搜索结果界面)
            - [6.1.2.1. 导航条部分](#6121-导航条部分)
            - [6.1.2.2. 搜索结果部分](#6122-搜索结果部分)
            - [6.1.2.3. 信息展示部分](#6123-信息展示部分)
    - [6.2. 软件接口](#62-软件接口)
    - [6.3. 故障处理](#63-故障处理)
- [7. 其他需求(检测或验收标准](#7-其他需求检测或验收标准)

<!-- /TOC -->

# 1. 引言

## 1.1. 编写目的

复习课本知识，锻炼实战能力，培养团队协作精神

## 1.2. 项目背景

《信息内容安全》网络信息内容获取技术课程项目设计
- 一个至少能支持10个以上网站的爬虫程序，且支持增量式数据采集;并至少采集10000个实际网页;
- 针对采集回来的网页内容， 能够实现网页文本的分类;
- 可进行重复或冗余网页的去重过滤;
- 对经去冗以后的内容建立倒排索引;
- 采用PageRank算法实现搜索结果的排序;
- 支持自然语言的模糊检索;
- 可实现搜索结果的可视化呈现。
- 可以在线记录每次检索的日志，井可对日志数据进统计分析和关联挖掘。

## 1.3. 定义(专门术语和缩写词)

|术语|含义|缩写词|
|-|-|-|
|ElasticSearch||ES、es|

# 2. 任务概述
## 2.1. 目标

实现一个以 ElasticSearch 为基础，网页前端作为用户界面的的搜索引擎系统。要求一个高效稳定的爬虫程序以及以 paddlepaddle 为基础的深度学习模块进行文本分类。

## 2.2. 运行环境

- 全平台：Windows、Mac、Linux
- python 3.7
- jdk 1.8.0
- ElasticSearch 7.4.0

## 2.3. 条件限制

- 可在 Intel Core i7-6700 CPU 、 16GB 内存的硬件条件及以上运行
- 开发时间 3 周内

# 3. 数据描述
## 3.1. 静态数据

|变量名|描述|
|:-:|:-:|
|`thread_accoun`|线程个数|
|`initial_url`|种子页面|

## 3.2. 动态数据

|变量名|描述|类型|
|:-:|:-:|:-:|
|`restricted_domain`|限定域名|列表|
|`banned_domain`|禁止域名|列表|
|`thread_account`|线程个数|整型|
|`total_pages`|限定页面个数|整型|

## 3.3. ES索引描述

ES “索引”类同数据库中的“表”，限定了数据项的类型。此索引记录了一个页面的一些必要信息，详情请见[3.4 索引数据字典](#34-索引数据字典)。通过发送如下请求，即可在ES中创建名为`page`，类型为`_type`的索引。

```
PUT http://127.0.0.1:9200/page
{
    "settings": {
        "number_of_shards": "5",
        "number_of_replicas": "0"
    },
    "mappings": {
        "properties": {
            "title": {
                "type": "text",
                "analyzer": "ik_max_word"
            },
            "weight": {
                "type": "double"
            },
            "content" : {
                "type" : "text",
                "analyzer": "ik_max_word"
            },
            "content_type": {
                "type": "text"
            },
            "url": {
                "type": "text",
                "analyzer": "ik_max_word"
            },
            "update_date": {
                "type": "date",
                "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
            }
        }
    }
}
```

## 3.4. 索引数据字典

页面（page）信息索引：
|数据项名称|含义|别名|类型|备注|
|-|-|-|-|-|
|`title`|网站标题||`text`|使用`ik_max_word`分词|
|`weight`|PageRank值|pr值，PR值|`double`||
|`content`|网站中的内容||`text`|使用`ik_max_word`分词|
|`content_type`|网站中的内容分类||`text`|文化, 娱乐, 体育, 财经, 房产, 汽车, 教育, 科技, 国际, 证券|
|`url`|网站的链接||`text`|使用`ik_max_word`分词|
|`update_date`|数据更新的时间||`date`|`yyyy-MM-dd HH:mm:ss`\|\|`yyyy-MM-dd`\|\|`epoch_millis`|

## 3.5. 数据采集

种子 url 数据从 init_url 列表中选取，并按照顺序，依次以各个 url 为起点进行递归的数据采集
爬取数据的url需要限制在 restricted_url 列表里面

# 4. 功能需求

## 4.1. 数据爬取与预处理功能

利用Python爬虫，爬取网站如下信息，详细描述见[3.4. 索引数据字典](#34-索引数据字典)
- title
- content
- content_type
- update_date
- url
- link（当前网站中包含的所有链接，用于计算pr值）

## 4.2. 计算 PageRank 功能

根据`link`计算爬取下来的每个网站的PageRank值，作为网站重要程度的指标，并补充到网站信息中

## 4.3. AI 文本分类并提交到 ES 功能

利用深度学习，分析每个页面的content的类别。将类别补充到网站信息中，同时删除网站信息中不再使用的`link`项，形成最终数据（参考[3.4. 索引数据字典](#34-索引数据字典)），并上传至ES，供用户交互功能调用。
    
## 4.4. 用户交互功能

设计WebApp，用户通过浏览器访问页面。用户提交搜索信息后，判断合法性，不合法则返回ERROR界面提示用户。如果合法，则后端代码从本地 ES 中查询数据，处理后将结果分条显示到前端。同时通过限制单个ip每分钟的访问次数来简单防御用户恶意搜索。

# 5. 性能需求

## 5.1. 数据精确度


## 5.2. 时间特性

## 5.3. 适应性

# 6. 运行需求

## 6.1. 用户界面

用户通过浏览器访问，有两个页面，一个是主页，只有简单的输入框提供用户搜索；另一个是一般界面，提供高级搜索功能，并显示搜索结果。

### 6.1.1. 主页

|控件|作用|布局|
|-|-|-|
|图标|显示Logo|居中|
|输入框|接收用户输入的关键字|Logo图标下偏左
|按钮|提交用户输入的关键字，并返回搜索结果|输入框右|

### 6.1.2. 搜索结果界面
该界面分为三个部分，导航条、搜索结果、信息展示。这三个部分布局如下

|部分|位置|height|width|
|-|-|-|-|
|导航条|顶部|50px|100%|
|搜索结果|导航条左下部|auto|70%|
|信息展示|导航条右下部|auto|30%|

#### 6.1.2.1. 导航条部分
以下控件从左向右依次（顺序可以任意）在导航条中排列

|控件|作用|
|-|-|
|输入框|接收用户输入的关键字
|输入框|可以输入域名，将搜索结果限制在该域名内
|数字输入框|查询结果分页显示，该框指示跳转到指定的搜索结果页
|选择框|允许用户选择匹配方式：标题和内容（默认）、仅标题、仅内容
|选择框|选择搜索结果的排序方式：倒排索引（默认）、 PageRank 排序
|按钮|提交用户输入的所有数据，并返回搜索结果

#### 6.1.2.2. 搜索结果部分

将搜索结果以list的形式展示出来，每个list item显示匹配的网站的如下数据
- 标题
- 内容
- url
- 类别
- PageRank值
- 更新时间

在list结尾，显示分页组件，使用户可以点击跳转，样式如下：
<table >
    <tr>
        <td><</td>
        <td>1</td>
        <td>2</td>
        <td>3</td>
        <td style="background-color:#337ab7;">4</td>
        <td>5</td>
        <td>6</td>
        <td>></td>
    </tr>
</table>

#### 6.1.2.3. 信息展示部分

展示一些必要信息，如：
- 本次查询耗时
- 查询结果数
- 数据库中的数据总数
- 等等

## 6.2. 软件接口

|接口名|描述|所在模块|调用方式|
|:-:|:-:|:-:|:-:|
|`init_first_time()`|初次启动调用此接口|`crawler.py`|内部调用|
|`get_result(url)`|得到目标 url 的页面|`crawler.py`|内部调用|
|`spider_thread()`|爬虫线程|`crawler.py`|内部调用|
|`main()`|主任务执行线程|`crawler.py`|`crawler.main()`|
|`init()`|去掉所有未在 url 中出现的 link 及错误文件|`PageRank.py`|内部调用|
|`Rank(Value, start)`|计算PageRank|`PageRank.py`|内部调用|
|`run()`|程序运行方法|`PageRank.py`|`PageRank.run()`|
|`get_data(sentence)`|获取已爬取数据|`Classify.py`|内部调用|
|`batch_reader(json_list,json_path)`|利用AI进行文本分类|`Classify.py`|`Classify.batch_reader()`|


## 6.3. 故障处理
# 7. 其他需求(检测或验收标准
- 可用性、可维护性、可移植性、安全保密性)
